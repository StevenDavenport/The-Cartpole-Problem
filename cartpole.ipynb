{"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"orig_nbformat":2,"kernelspec":{"name":"python385jvsc74a57bd07e7b17f9149d334c2c83f45e351d9baee7fbe1bd9547b2a9ec3772ffc05b99f3","display_name":"Python 3.8.5 64-bit ('gym': conda)"},"colab":{"name":"cartpole.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"5SZSXlKS0shm","executionInfo":{"status":"ok","timestamp":1630916025883,"user_tz":-60,"elapsed":621,"user":{"displayName":"Steven Davenport","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhlJZYzSG_jq91CN1e6WMtz7TBWXu_u41mLUJTi=s64","userId":"04887223458757494819"}}},"source":["# Keras imports \n","from keras.layers import Input, Dense\n","from keras.models import Model, load_model, Sequential\n","from keras.optimizers import rmsprop_v2\n","\n","# gym imports\n","import gym\n","from gym.envs.classic_control.cartpole import *\n","\n","# utility imports\n","import os\n","import numpy as np\n","from collections import deque\n","from itertools import count\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","import time"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"W5X4xkO20shx","executionInfo":{"status":"ok","timestamp":1630916046838,"user_tz":-60,"elapsed":570,"user":{"displayName":"Steven Davenport","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhlJZYzSG_jq91CN1e6WMtz7TBWXu_u41mLUJTi=s64","userId":"04887223458757494819"}}},"source":["class Agent():\n","    def __init__(self, env):\n","        self.env = env\n","        self.observation_size = self.env.observation_space.shape[0]\n","        self.action_size = self.env.action_space.n\n","        self.memory = deque(maxlen=2000)    # relay memory\n","        self.learning_rate = 0.00025        # learning rate\n","        self.epsilon = 1.0                  # epsilon, starting at max\n","        self.epsilon_min = 0.001            # end value of epsilon\n","        self.epsilon_decay = 0.999          # epsilon decay rate\n","        self.gamma = 0.95                   # gamma value (used in belmans)\n","        self.batch_size = 64                # training batch size\n","        self.train_start = 1000             # steps before training begins\n","        self.steps_taken = 0                # total steps taken\n","        self.update_target = 20             # steps before target model update\n","        self.moving_avg_period = 25         # moving average, used for graphing\n","        self.moving_avg = []                # moving average\n","        self.episode_rewards = []           # total reward from each episode\n","        self.model = None                   # policy network\n","        self.target_model = None            # target network\n","        self.score_save_limit = 800         # any model score > 800 = save\n","\n","    \"\"\"\n","    Returns a fully connected network.\n","    Used as the policy and target network.    \n","    \"\"\"\n","    def build_model(self):\n","        model = Sequential()\n","        model.add(Dense(512, input_dim=self.observation_size, activation=\"relu\", kernel_initializer='he_uniform'))\n","        model.add(Dense(256, activation=\"relu\", kernel_initializer='he_uniform'))\n","        model.add(Dense(64, activation=\"relu\", kernel_initializer='he_uniform'))\n","        model.add(Dense(self.action_size, activation=\"linear\", kernel_initializer='he_uniform'))\n","        model.compile(loss=\"mse\", optimizer=rmsprop_v2.RMSprop(learning_rate=self.learning_rate, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n","        return model\n","    \n","    \"\"\"\n","    Returns the policy networks prediction of the \n","    best action to take.\n","    Used when not training. \n","    \"\"\"\n","    def take_action(self, state):\n","        if self.model == None: \n","            self.model = load_model('C:/Users/Steven/Desktop/cartpole_solution/cartpole/models/model53k.h5')\n","        state = np.squeeze(state).reshape(1,4)\n","        return np.argmax(self.model.predict(state))     # Exploit Enviroment\n","\n","    \"\"\"\n","    Plots the results of the training steps \n","    and a moving average of a given size,\n","    live while the agent is learing and\n","    playing.\n","    \"\"\"\n","    def plot(self):\n","        plt.figure(1)\n","        plt.clf()        \n","        plt.title('Training...')\n","        plt.xlabel('Episode')\n","        plt.ylabel('Duration')\n","        self.moving_avg.append(sum(self.episode_rewards[-self.moving_avg_period:]) // self.moving_avg_period if len(self.episode_rewards) >= self.moving_avg_period else 0)\n","        plt.plot(self.episode_rewards)\n","        plt.plot(self.moving_avg)    \n","        plt.pause(0.001)\n","        clear_output(wait=True)\n","\n","    \"\"\"\n","    Function plays the cartpole game and an agent is trained\n","    and saved in the process, using deep q learning. \n","    \"\"\"\n","    def learn(self, episodes):\n","        self.model = self.build_model()\n","        self.target_model = self.build_model()\n","\n","        for episode in range(episodes):\n","            state = self.env.reset()\n","            state = np.reshape(state, [1, self.observation_size])\n","            done = False\n","            for step in count():\n","                self.env.render()\n","                # Decide an action\n","                action = None\n","                if random.random() <= self.epsilon:\n","                    action = random.randrange(self.action_size)\n","                else:\n","                    action = np.argmax(self.model.predict(state))\n","                # Make an action\n","                next_state, reward, done, _ = self.env.step(action)\n","                next_state = np.reshape(next_state, [1, self.observation_size])\n","                if not done: \n","                    reward = reward\n","                else:\n","                    reward = -100\n","                # Add experience to replay memory\n","                self.memory.append((state, action, reward, next_state, done))\n","                state = next_state\n","                # Update Epsilon\n","                if len(self.memory) > self.train_start:\n","                    if self.epsilon > self.epsilon_min:\n","                        self.epsilon *= self.epsilon_decay\n","                # Update Target Network\n","                if self.steps_taken % self.update_target == 0: \n","                    self.target_model.set_weights(self.model.get_weights())   \n","                # Check if episode is done\n","                if done:                   \n","                    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(episode, episodes, step, self.epsilon))\n","                    self.episode_rewards.append(step)\n","                    self.plot()\n","                    if step >= self.score_save_limit:\n","                        self.model.save('best_model_800.h5')\n","                    break\n","                # Create a batch from replay memory\n","                if len(self.memory) >= self.train_start:\n","                    batch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n","                    state_batch, next_state_batch = [], []\n","                        # put states and next states in a list\n","                    for _state, _action, _reward, _next_state, _done in batch:\n","                        state_batch.append(_state)\n","                        next_state_batch.append(_next_state)\n","                        # get the reward and next rewards\n","                    state_batch = np.array(state_batch).reshape(self.batch_size, self.observation_size)\n","                    next_state_batch = np.array(next_state_batch).reshape(self.batch_size, self.observation_size)\n","                    target = self.model.predict(state_batch)\n","                    target_next = self.target_model.predict(next_state_batch)\n","                        # get q values\n","                    x = 0\n","                    for _state, _action, _reward, _next_state, _done in batch:\n","                        if _done:\n","                            target[x][_action] = _reward\n","                        else:\n","                            target[x][_action] = _reward + self.gamma * (np.amax(target_next[x]))\n","                        x += 1\n","                    # Train the network\n","                    self.model.fit(state_batch, target, batch_size=self.batch_size, verbose=0)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"F5XV8ucQ0siD","outputId":"293dfb06-46db-4d04-9cb8-65c9c244559e"},"source":["#This is the mannual game play cell\n","import gym\n","from gym.envs.classic_control.cartpole import *\n","from pyglet.window import key\n","import matplotlib.pyplot as plt\n","import time\n","\n","bool_do_not_quit = True  # Boolean to quit pyglet\n","scores = []  # Your gaming score\n","a = 0  # Action\n","\n","def key_press(k, mod):\n","    global bool_do_not_quit, a, restart\n","    if k==0xff0d: restart = True\n","    if k==key.ESCAPE: bool_do_not_quit=False  # Added to Quit\n","    if k==key.Q: bool_do_not_quit=False  # Added to Quit\n","    if k==key.LEFT:  a = 0  # 0     Push cart to the left\n","    if k==key.RIGHT: a = 1  # 1     Push cart to the right\n","\n","def run_cartPole_asHuman(policy=None):\n","    env = CartPoleEnv()\n","\n","    env.reset()\n","    env.render()\n","    env.viewer.window.on_key_press = key_press\n","\n","    while bool_do_not_quit:\n","        env.reset()\n","        total_reward = 0.0\n","        steps = 0\n","        restart = False\n","        t1 = time.time()  # Trial timer\n","        while bool_do_not_quit:\n","            s, r, done, info = env.step(a)\n","            time.sleep(1/10)  # 10fps: Super slow for us poor little human!\n","            total_reward += r\n","            steps += 1\n","        \n","            env.render()\n","            if done or restart:\n","                t1 = time.time()-t1\n","                scores.append(total_reward)\n","                print(\"Episode\", len(scores), \"| Score:\", total_reward, '|', steps, \"steps | %0.2fs.\"% t1)\n","                break\n","    env.close()\n","\n","\n","run_cartPole_asHuman() \n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import statistics\n","print(\"Average Score for \", len(scores), \" episodes is:\",statistics.mean(scores))\n","# Plot your score\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","plt.plot(np.arange(1, len(scores)+1), scores)\n","plt.title('My human performance on CartPole-v0')\n","plt.ylabel('Score')\n","plt.xlabel('Human Episode')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Episode 1 | Score: 10.0 | 10 steps | 1.18s.\n","Episode 2 | Score: 9.0 | 9 steps | 1.06s.\n","Episode 3 | Score: 9.0 | 9 steps | 1.05s.\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-6-7b1317b72be7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0mrun_cartPole_asHuman\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-6-7b1317b72be7>\u001b[0m in \u001b[0;36mrun_cartPole_asHuman\u001b[1;34m(policy)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mbool_do_not_quit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 10fps: Super slow for us poor little human!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0msteps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"OsL40_F-0siO","colab":{"base_uri":"https://localhost:8080/","height":504},"executionInfo":{"status":"error","timestamp":1630916246328,"user_tz":-60,"elapsed":237,"user":{"displayName":"Steven Davenport","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhlJZYzSG_jq91CN1e6WMtz7TBWXu_u41mLUJTi=s64","userId":"04887223458757494819"}},"outputId":"d49dcc70-bbb9-433d-c6f0-ca6b42989f30"},"source":["import gym\n","from gym.envs.classic_control.cartpole import *\n","from pyglet.window import key\n","import matplotlib.pyplot as plt\n","import time\n","\n","\n","bool_do_not_quit = True  # Boolean to quit pyglet\n","scores = []  # Your gaming score\n","a = 0  # Action\n","env = CartPoleEnv()\n","\n","number_of_trials = 50\n","\n","def key_press(k, mod):\n","    global bool_do_not_quit, a, restart\n","    if k==0xff0d: restart = True\n","    if k==key.ESCAPE: bool_do_not_quit=False  # Added to Quit\n","    if k==key.Q: bool_do_not_quit=False  # Added to Quit\n","    if k==key.LEFT:  a = 0  # 0     Push cart to the left\n","    if k==key.RIGHT: a = 1  # 1     Push cart to the right\n","\n","\n","def run_cartPole_asAgent(agent):\n","\n","    env.reset()\n","    env.render()\n","    env.viewer.window.on_key_press = key_press\n","\n","    for _ in range(number_of_trials):\n","        state = env.reset()\n","        total_reward = 0.0\n","        steps = 0\n","        restart = False\n","        t1 = time.time()  # Trial timer\n","        while bool_do_not_quit:\n","            #this is where policy function outputs action a based on the current state\n","            action = agent.take_action(state)\n","            #this is there you get the next system state after take action a\n","            state, reward, done, info = env.step(action)\n","            time.sleep(1/10)  # 10fps: Super slow for us poor little human!\n","            total_reward += reward\n","            steps += 1\n","            env.render()\n","            if done or restart:\n","                t1 = time.time()-t1\n","                scores.append(total_reward)\n","                print(\"Episode\", len(scores), \"| Score:\", total_reward, '|', steps, \"steps | %0.2fs.\"% t1)\n","                break\n","    env.close()\n","\n","agent = Agent(env)\n","run_cartPole_asAgent(agent)  # Run with agent input\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import statistics\n","print(\"Average Score for \", number_of_trials, \" episodes is:\",statistics.mean(scores))\n","# Plot your score\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","plt.plot(np.arange(1, len(scores)+1), scores)\n","plt.title('My agent performance on CartPole-v0')\n","plt.ylabel('Score')\n","plt.xlabel('Agent Episode')\n","plt.show()"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function Viewer.__del__ at 0x7f31a8544cb0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/rendering.py\", line 165, in __del__\n","    self.close()\n","  File \"/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/rendering.py\", line 81, in close\n","    if self.isopen and sys.meta_path:\n","AttributeError: 'Viewer' object has no attribute 'isopen'\n"]},{"output_type":"error","ename":"NoSuchDisplayException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-e55957170ad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcartpole\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_doc_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""]}]}]}